# Interesting Papers


## World Models

- [World Models](https://arxiv.org/abs/1803.10122), web: [Website](https://worldmodels.github.io/)
- MuZero: [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265)
- MuZero ReAnalyse [Online and Offline Reinforcement Learning by Planning with a Learned Model](https://arxiv.org/abs/2104.06294)
- Sampled MuZero: [Learning and Planning in Complex Action Spaces](https://arxiv.org/abs/2104.06303)
- EfficientZero: [Mastering Atari Games with Limited Data](https://arxiv.org/abs/2111.00210)
- [Procedural Generalization by Planning with Self-Supervised World Models](https://arxiv.org/abs/2111.01587)  (planning, self-supervised learning, data diversity)
- DreamerV2: [Mastering Atari with Discrete World Models](https://arxiv.org/abs/2010.02193)
- Dreamer: [Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603)
- VQM: [Vector Quantized Models for Planning](https://arxiv.org/abs/2106.04615), video: [Video](https://sites.google.com/view/vqmodels/home)
- PlaNet/RSSM: [Learning Latent Dynamics for Planning from Pixels](https://arxiv.org/abs/1811.04551)
- [Smaller World Models for Reinforcement Learning](https://arxiv.org/abs/2010.05767)
- [Learning to drive from a world on rails](https://arxiv.org/abs/2105.00636)
- GameGAN: [Learning to Simulate Dynamic Environments with GameGAN](https://arxiv.org/abs/2005.12126), web: [Website](https://nv-tlabs.github.io/gameGAN/)
- DriveGAN: [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/abs/2104.15060), web: [Website](https://nv-tlabs.github.io/DriveGAN/)
- OP3: [Entity Abstraction in Visual Model-Based Reinforcement Learning](https://arxiv.org/abs/1910.12827)
- AWML: [Active World Model Learning with Progress Curiosity](https://arxiv.org/abs/2007.07853)
- SCOFF: [Object Files and Schemata: Factorizing Declarative and Procedural Knowledge in Dynamical Systems](https://arxiv.org/abs/2006.16225)
- PVG/CADDY: [Playable Video Generation](https://arxiv.org/abs/2101.12195), web: [Website](https://willi-menapace.github.io/playable-video-generation-website/), code: [willi-menapace/PlayableVideoGeneration](https://github.com/willi-menapace/PlayableVideoGeneration)
- LEXA: [Discovering and Achieving Goals via World Models](https://arxiv.org/abs/2110.09514), web: [Website](https://orybkin.github.io/lexa/), code: [orybkin/lexa](https://github.com/orybkin/lexa)
- Plan2Explore: [Planning to Explore via Self-Supervised World Models](https://arxiv.org/abs/2005.05960), web: [Website](https://ramanans1.github.io/plan2explore/), code: [ramanans1/plan2explore](https://github.com/ramanans1/plan2explore)
- [Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents](https://arxiv.org/abs/2201.07207), web: [Website](https://wenlong.page/language-planner/), code: [huangwl18/language-planner](https://github.com/huangwl18/language-planner), video: [Author interview](https://youtu.be/OUCwujwE7bA) (by Y. Kilcher)


## Video Prediction / Generation

- CogVideo: [CogVideo: Large-scale Pretraining for Text-to-Video Generation via Transformers](https://raw.githubusercontent.com/THUDM/CogVideo/main/paper/CogVideo-arxiv.pdf), web: [GH page](https://github.com/THUDM/CogVideo)
- TATS (3D VQGAN): [Long Video Generation with Time-Agnostic VQGAN and Time-Sensitive Transformer](https://arxiv.org/abs/2204.03638), web: [Website](https://songweige.github.io/projects/tats/index.html)
- FDM: [Flexible Diffusion Modeling of Long Videos](https://arxiv.org/abs/2205.11495)
- VDM: [Video Diffusion Models](https://arxiv.org/abs/2204.03458), web: [Website](https://video-diffusion.github.io/)
- MSPred: [Video Prediction at Multiple Scales with Hierarchical Recurrent Networks](https://arxiv.org/abs/2203.09303), web: [Website](https://sites.google.com/view/mspred/home)
- 3DNA: [NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion](https://arxiv.org/abs/2111.12417), web: [Website](https://github.com/microsoft/NUWA)
- VideoGPT: [VideoGPT: Video Generation using VQ-VAE and Transformers](https://arxiv.org/abs/2104.10157)
- FitVid: [FitVid: Overfitting in Pixel-Level Video Prediction](https://arxiv.org/abs/2106.13195)
- [Predicting Video with VQVAE](https://arxiv.org/abs/2103.01950)
- [Clockwork Variational Autoencoders](https://arxiv.org/abs/2102.09532)
- GHVAE: [Greedy Hierarchical Variational Autoencoders for Large-Scale Video Prediction](https://arxiv.org/abs/2103.04174), web: [Website](https://sites.google.com/view/ghvae)
- StyleGAN-V: [StyleGAN-V: A Continuous Video Generator with the Price, Image Quality and Perks of StyleGAN2](https://arxiv.org/abs/2112.14683), web: [Website](https://universome.github.io/stylegan-v)
- MAU: [MAU: A Motion-Aware Unit for Video Prediction and Beyond](https://proceedings.neurips.cc/paper/2021/hash/e25cfa90f04351958216f97e3efdabe9-Abstract.html)


## RL

- Diffuser: [Planning with Diffusion for Flexible Behavior Synthesis](https://arxiv.org/abs/2205.09991), web: [Website](https://diffusion-planning.github.io/), code: [jannerm/diffuser](https://github.com/jannerm/diffuser)
- [Model-based Reinforcement Learning: A Survey](https://arxiv.org/abs/2006.16712v3)
- relational unit (self-attention): [Relational Deep Reinforcement Learning](https://arxiv.org/abs/1806.01830)
- XLand: [Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808), [Blog](https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play), [Video](https://youtu.be/lTmL7jwFfdw)
- [Survey of Generalisation in Deep Reinforcement Learning](https://arxiv.org/abs/2111.09794)
- Embodiment: [From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence](https://arxiv.org/abs/2110.15245)


## Important Related Papers

- VQ-VAE-2: [Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446), code: [rosinality/vq-vae-2-pytorch](https://github.com/rosinality/vq-vae-2-pytorch)
- PixelCNN: [Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)
- PixelSNAIL: [PixelSNAIL: An Improved Autoregressive Generative Model](https://arxiv.org/abs/1712.09763)
- VQGAN: [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841), web: [Website](https://compvis.github.io/taming-transformers/)
- DALL-E [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092), code: [sberbank-ai/ru-dalle](https://github.com/sberbank-ai/ru-dalle/tree/master/rudalle)
- VQ-Diffusion: [Vector Quantized Diffusion Model for Text-to-Image Synthesis](https://arxiv.org/abs/2111.14822), code: [microsoft/VQ-Diffusion](https://github.com/microsoft/VQ-Diffusion)


## New Methods Under Investigation
- S4: [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396), [Video](https://www.youtube.com/watch?v=EvQ3ncuriCM), code: [HazyResearch/state-spaces](https://github.com/HazyResearch/state-spaces)
- SRU++: [When Attention Meets Fast Recurrence:Training Language Models with Reduced Compute](https://arxiv.org/abs/2102.12459), web: [Website](https://www.asapp.com/blog/reducing-the-high-cost-of-training-nlp-models-with-sru/), code: [asappresearch/sru](https://github.com/asappresearch/sru)



## Interesting Other Stuff

- Papers With Code: [Video Prediction](https://paperswithcode.com/task/video-prediction)
- Blog: [World Models (the long version)](https://adgefficiency.com/world-models/)
- Blog: [Debugging Deep Model-based Reinforcement Learning Systems](https://www.natolambert.com/writing/debugging-mbrl)
- Blog: [A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning](https://mila.quebec/en/article/a-consciousness-inspired-planning-agent-for-model-based-reinforcement-learning/)
- Blog: [World Models applied to Sonic](https://dylandjian.github.io/world-models/), code: [dylandjian/retro-contest-sonic](https://github.com/dylandjian/retro-contest-sonic)
- Project: [Cracking PHYRE with a World Model](https://cse.buffalo.edu/~avereshc/rl_spring20/Sheng_Liu.pdf)
- [Free Will Belief as a consequence of Model-based Reinforcement Learning](https://arxiv.org/abs/2111.08435)


## Acknowledgements

I would like to thank the following people for suggesting papers and links:
- eop
- Alexander Nikolin
- jat
- OGeneral
- augustocsc