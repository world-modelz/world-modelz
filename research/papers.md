# Interesting Papers


## World Models

- [World Models](https://arxiv.org/abs/1803.10122), [Website](https://worldmodels.github.io/)
- MuZero: [Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model](https://arxiv.org/abs/1911.08265)
- MuZero ReAnalyse [Online and Offline Reinforcement Learning by Planning with a Learned Model](https://arxiv.org/abs/2104.06294)
- Sampled MuZero: [Learning and Planning in Complex Action Spaces](https://arxiv.org/abs/2104.06303)
- EfficientZero: [Mastering Atari Games with Limited Data](https://arxiv.org/abs/2111.00210)
- [Procedural Generalization by Planning with Self-Supervised World Models](https://arxiv.org/abs/2111.01587)  (planning, self-supervised learning, data diversity)
- DreamerV2: [Mastering Atari with Discrete World Models](https://arxiv.org/abs/2010.02193)
- Dreamer: [Dream to Control: Learning Behaviors by Latent Imagination](https://arxiv.org/abs/1912.01603)
- VQM: [Vector Quantized Models for Planning](https://arxiv.org/abs/2106.04615), [Video](https://sites.google.com/view/vqmodels/home)
- PlaNet/RSSM: [Learning Latent Dynamics for Planning from Pixels](https://arxiv.org/abs/1811.04551)
- [Smaller World Models for Reinforcement Learning](https://arxiv.org/abs/2010.05767)
- [Learning to drive from a world on rails](https://arxiv.org/abs/2105.00636)
- GameGAN: [Learning to Simulate Dynamic Environments with GameGAN](https://arxiv.org/abs/2005.12126), [Website](https://nv-tlabs.github.io/gameGAN/)
- DriveGAN: [DriveGAN: Towards a Controllable High-Quality Neural Simulation](https://arxiv.org/abs/2104.15060), [Website](https://nv-tlabs.github.io/DriveGAN/)
- OP3: [Entity Abstraction in Visual Model-Based Reinforcement Learning](https://arxiv.org/abs/1910.12827)
- AWML: [Active World Model Learning with Progress Curiosity](https://arxiv.org/abs/2007.07853)
- SCOFF: [Object Files and Schemata: Factorizing Declarative and Procedural Knowledge in Dynamical Systems](https://arxiv.org/abs/2006.16225)
- PVG/CADDY: [Playable Video Generation](https://arxiv.org/abs/2101.12195), [Website](https://willi-menapace.github.io/playable-video-generation-website/), code [willi-menapace/PlayableVideoGeneration](https://github.com/willi-menapace/PlayableVideoGeneration)


### Philosophic
- [Free Will Belief as a consequence of Model-based Reinforcement Learning](https://arxiv.org/abs/2111.08435)


## Video prediction

- 3DNA: [NÃœWA: Visual Synthesis Pre-training for Neural visUal World creAtion](https://arxiv.org/abs/2111.12417), [Website](https://github.com/microsoft/NUWA)
- VideoGPT: [VideoGPT: Video Generation using VQ-VAE and Transformers](https://arxiv.org/abs/2104.10157)
- FitVid: [FitVid: Overfitting in Pixel-Level Video Prediction](https://arxiv.org/abs/2106.13195)
- [Predicting Video with VQVAE](https://arxiv.org/abs/2103.01950)
- [Clockwork Variational Autoencoders](https://arxiv.org/abs/2102.09532)


## RL

- relational unit (self-attention): [Relational Deep Reinforcement Learning](https://arxiv.org/abs/1806.01830)
- XLand: [Open-Ended Learning Leads to Generally Capable Agents](https://arxiv.org/abs/2107.12808), [Blog](https://deepmind.com/blog/article/generally-capable-agents-emerge-from-open-ended-play), [Video](https://youtu.be/lTmL7jwFfdw)
- [Survey of Generalisation in Deep Reinforcement Learning](https://arxiv.org/abs/2111.09794)
- Embodiment: [From Machine Learning to Robotics: Challenges and Opportunities for Embodied Intelligence](https://arxiv.org/abs/2110.15245)


## Important Related Papers

- VQ-VAE-2: [Generating Diverse High-Fidelity Images with VQ-VAE-2](https://arxiv.org/abs/1906.00446), code: [rosinality/vq-vae-2-pytorch](https://github.com/rosinality/vq-vae-2-pytorch)
- PixelCNN: [Conditional Image Generation with PixelCNN Decoders](https://arxiv.org/abs/1606.05328)
- PixelSNAIL: [PixelSNAIL: An Improved Autoregressive Generative Model](https://arxiv.org/abs/1712.09763)
- VQGAN: [Taming Transformers for High-Resolution Image Synthesis](https://arxiv.org/abs/2012.09841), [Website](https://compvis.github.io/taming-transformers/)
- DALL-E [Zero-Shot Text-to-Image Generation](https://arxiv.org/abs/2102.12092), code: [sberbank-ai/ru-dalle](https://github.com/sberbank-ai/ru-dalle/tree/master/rudalle)
- VQ-Diffusion: [Vector Quantized Diffusion Model for Text-to-Image Synthesis](https://arxiv.org/abs/2111.14822), code: [microsoft/VQ-Diffusion](https://github.com/microsoft/VQ-Diffusion)


## New Methods Under Investigation
- S4: [Efficiently Modeling Long Sequences with Structured State Spaces](https://arxiv.org/abs/2111.00396), [Video](https://www.youtube.com/watch?v=EvQ3ncuriCM), code: [HazyResearch/state-spaces](https://github.com/HazyResearch/state-spaces)
- SRU++: [When Attention Meets Fast Recurrence:Training Language Models with Reduced Compute](https://arxiv.org/abs/2102.12459), [Website](https://www.asapp.com/blog/reducing-the-high-cost-of-training-nlp-models-with-sru/), code: [asappresearch/sru](https://github.com/asappresearch/sru)


## Interesting Other Stuff

- Blog: [World Models (the long version)](https://adgefficiency.com/world-models/)
- Blog: [Debugging Deep Model-based Reinforcement Learning Systems](https://www.natolambert.com/writing/debugging-mbrl)
- Blog: [A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning](https://mila.quebec/en/article/a-consciousness-inspired-planning-agent-for-model-based-reinforcement-learning/)
- Blog: [World Models applied to Sonic](https://dylandjian.github.io/world-models/), code: [dylandjian/retro-contest-sonic](https://github.com/dylandjian/retro-contest-sonic)
- Project: [Cracking PHYRE with a World Model](https://cse.buffalo.edu/~avereshc/rl_spring20/Sheng_Liu.pdf)


## Acknowledgements

I would like to thank the following people for suggesting papers and links:
- eop
- Alexander Nikolin
- jat
- OGeneral
